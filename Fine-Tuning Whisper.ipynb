{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3d7bbcf",
   "metadata": {},
   "source": [
    "# Preparing Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41920a87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /home/user1/anaconda3/envs/py39/lib/python3.9/site-packages (24.2)\n",
      "Requirement already satisfied: transformers in /home/user1/anaconda3/envs/py39/lib/python3.9/site-packages (4.43.4)\n",
      "Requirement already satisfied: accelerate in /home/user1/anaconda3/envs/py39/lib/python3.9/site-packages (0.33.0)\n",
      "Requirement already satisfied: evaluate in /home/user1/anaconda3/envs/py39/lib/python3.9/site-packages (0.4.2)\n",
      "Requirement already satisfied: jiwer in /home/user1/anaconda3/envs/py39/lib/python3.9/site-packages (3.0.4)\n",
      "Requirement already satisfied: tensorboard in /home/user1/anaconda3/envs/py39/lib/python3.9/site-packages (2.17.0)\n",
      "Requirement already satisfied: gradio in /home/user1/anaconda3/envs/py39/lib/python3.9/site-packages (4.40.0)\n",
      "Requirement already satisfied: datasets[audio] in /home/user1/anaconda3/envs/py39/lib/python3.9/site-packages (2.20.0)\n",
      "Requirement already satisfied: filelock in /home/user1/anaconda3/envs/py39/lib/python3.9/site-packages (from datasets[audio]) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/user1/anaconda3/envs/py39/lib/python3.9/site-packages (from datasets[audio]) (1.26.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/user1/anaconda3/envs/py39/lib/python3.9/site-packages (from datasets[audio]) (17.0.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in /home/user1/anaconda3/envs/py39/lib/python3.9/site-packages (from datasets[audio]) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/user1/anaconda3/envs/py39/lib/python3.9/site-packages (from datasets[audio]) (0.3.8)\n",
      "Requirement already satisfied: pandas in /home/user1/anaconda3/envs/py39/lib/python3.9/site-packages (from datasets[audio]) (1.5.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /home/user1/anaconda3/envs/py39/lib/python3.9/site-packages (from datasets[audio]) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /home/user1/anaconda3/envs/py39/lib/python3.9/site-packages (from datasets[audio]) (4.66.5)\n",
      "Requirement already satisfied: xxhash in /home/user1/anaconda3/envs/py39/lib/python3.9/site-packages (from datasets[audio]) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /home/user1/anaconda3/envs/py39/lib/python3.9/site-packages (from datasets[audio]) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.5.0,>=2023.1.0 in /home/user1/anaconda3/envs/py39/lib/python3.9/site-packages (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets[audio]) (2023.10.0)\n",
      "Requirement already satisfied: aiohttp in /home/user1/anaconda3/envs/py39/lib/python3.9/site-packages (from datasets[audio]) (3.9.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.2 in /home/user1/anaconda3/envs/py39/lib/python3.9/site-packages (from datasets[audio]) (0.24.5)\n",
      "Requirement already satisfied: packaging in /home/user1/anaconda3/envs/py39/lib/python3.9/site-packages (from datasets[audio]) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/user1/anaconda3/envs/py39/lib/python3.9/site-packages (from datasets[audio]) (6.0.1)\n",
      "Requirement already satisfied: soundfile>=0.12.1 in /home/user1/anaconda3/envs/py39/lib/python3.9/site-packages (from datasets[audio]) (0.12.1)\n",
      "Requirement already satisfied: librosa in /home/user1/anaconda3/envs/py39/lib/python3.9/site-packages (from datasets[audio]) (0.10.2.post1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/user1/anaconda3/envs/py39/lib/python3.9/site-packages (from transformers) (2024.7.24)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/user1/anaconda3/envs/py39/lib/python3.9/site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /home/user1/anaconda3/envs/py39/lib/python3.9/site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: psutil in /home/user1/anaconda3/envs/py39/lib/python3.9/site-packages (from accelerate) (5.9.0)\n",
      "Requirement already satisfied: torch>=1.10.0 in /home/user1/anaconda3/envs/py39/lib/python3.9/site-packages (from accelerate) (2.1.0)\n",
      "Requirement already satisfied: click<9.0.0,>=8.1.3 in /home/user1/anaconda3/envs/py39/lib/python3.9/site-packages (from jiwer) (8.1.7)\n",
      "Requirement already satisfied: rapidfuzz<4,>=3 in /home/user1/anaconda3/envs/py39/lib/python3.9/site-packages (from jiwer) (3.9.5)\n",
      "Requirement already satisfied: absl-py>=0.4 in /home/user1/anaconda3/envs/py39/lib/python3.9/site-packages (from tensorboard) (2.0.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /home/user1/anaconda3/envs/py39/lib/python3.9/site-packages (from tensorboard) (1.59.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/user1/anaconda3/envs/py39/lib/python3.9/site-packages (from tensorboard) (3.5)\n",
      "Requirement already satisfied: protobuf!=4.24.0,<5.0.0,>=3.19.6 in /home/user1/anaconda3/envs/py39/lib/python3.9/site-packages (from tensorboard) (3.19.6)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /home/user1/anaconda3/envs/py39/lib/python3.9/site-packages (from tensorboard) (68.0.0)\n",
      "Requirement already satisfied: six>1.9 in /home/user1/anaconda3/envs/py39/lib/python3.9/site-packages (from tensorboard) (1.16.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /home/user1/anaconda3/envs/py39/lib/python3.9/site-packages (from tensorboard) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/user1/anaconda3/envs/py39/lib/python3.9/site-packages (from tensorboard) (3.0.0)\n",
      "Requirement already satisfied: aiofiles<24.0,>=22.0 in /home/user1/anaconda3/envs/py39/lib/python3.9/site-packages (from gradio) (22.1.0)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in /home/user1/anaconda3/envs/py39/lib/python3.9/site-packages (from gradio) (3.5.0)\n",
      "Requirement already satisfied: fastapi in /home/user1/anaconda3/envs/py39/lib/python3.9/site-packages (from gradio) (0.112.0)\n",
      "Requirement already satisfied: ffmpy in /home/user1/anaconda3/envs/py39/lib/python3.9/site-packages (from gradio) (0.4.0)\n",
      "Requirement already satisfied: gradio-client==1.2.0 in /home/user1/anaconda3/envs/py39/lib/python3.9/site-packages (from gradio) (1.2.0)\n",
      "Requirement already satisfied: httpx>=0.24.1 in /home/user1/anaconda3/envs/py39/lib/python3.9/site-packages (from gradio) (0.27.0)\n",
      "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /home/user1/anaconda3/envs/py39/lib/python3.9/site-packages (from gradio) (6.1.0)\n",
      "Requirement already satisfied: jinja2<4.0 in /home/user1/anaconda3/envs/py39/lib/python3.9/site-packages (from gradio) (3.1.2)\n",
      "Requirement already satisfied: markupsafe~=2.0 in /home/user1/anaconda3/envs/py39/lib/python3.9/site-packages (from gradio) (2.1.1)\n",
      "Requirement already satisfied: matplotlib~=3.0 in /home/user1/anaconda3/envs/py39/lib/python3.9/site-packages (from gradio) (3.8.0)\n",
      "Requirement already satisfied: orjson~=3.0 in /home/user1/anaconda3/envs/py39/lib/python3.9/site-packages (from gradio) (3.10.6)\n",
      "Requirement already satisfied: pillow<11.0,>=8.0 in /home/user1/anaconda3/envs/py39/lib/python3.9/site-packages (from gradio) (10.1.0)\n",
      "Requirement already satisfied: pydantic>=2.0 in /home/user1/anaconda3/envs/py39/lib/python3.9/site-packages (from gradio) (2.8.2)\n",
      "Requirement already satisfied: pydub in /home/user1/anaconda3/envs/py39/lib/python3.9/site-packages (from gradio) (0.25.1)\n",
      "Requirement already satisfied: python-multipart>=0.0.9 in /home/user1/anaconda3/envs/py39/lib/python3.9/site-packages (from gradio) (0.0.9)\n",
      "Requirement already satisfied: ruff>=0.2.2 in /home/user1/anaconda3/envs/py39/lib/python3.9/site-packages (from gradio) (0.5.6)\n",
      "Requirement already satisfied: semantic-version~=2.0 in /home/user1/anaconda3/envs/py39/lib/python3.9/site-packages (from gradio) (2.10.0)\n",
      "Requirement already satisfied: tomlkit==0.12.0 in /home/user1/anaconda3/envs/py39/lib/python3.9/site-packages (from gradio) (0.12.0)\n",
      "Requirement already satisfied: typer<1.0,>=0.12 in /home/user1/anaconda3/envs/py39/lib/python3.9/site-packages (from gradio) (0.12.3)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in /home/user1/anaconda3/envs/py39/lib/python3.9/site-packages (from gradio) (4.12.2)\n",
      "Requirement already satisfied: urllib3~=2.0 in /home/user1/anaconda3/envs/py39/lib/python3.9/site-packages (from gradio) (2.2.2)\n",
      "Requirement already satisfied: uvicorn>=0.14.0 in /home/user1/anaconda3/envs/py39/lib/python3.9/site-packages (from gradio) (0.30.5)\n",
      "Requirement already satisfied: websockets<13.0,>=10.0 in /home/user1/anaconda3/envs/py39/lib/python3.9/site-packages (from gradio-client==1.2.0->gradio) (12.0)\n",
      "Requirement already satisfied: idna>=2.8 in /home/user1/anaconda3/envs/py39/lib/python3.9/site-packages (from anyio<5.0,>=3.0->gradio) (2.10)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/user1/anaconda3/envs/py39/lib/python3.9/site-packages (from anyio<5.0,>=3.0->gradio) (1.2.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/user1/anaconda3/envs/py39/lib/python3.9/site-packages (from aiohttp->datasets[audio]) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/user1/anaconda3/envs/py39/lib/python3.9/site-packages (from aiohttp->datasets[audio]) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/user1/anaconda3/envs/py39/lib/python3.9/site-packages (from aiohttp->datasets[audio]) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/user1/anaconda3/envs/py39/lib/python3.9/site-packages (from aiohttp->datasets[audio]) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/user1/anaconda3/envs/py39/lib/python3.9/site-packages (from aiohttp->datasets[audio]) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /home/user1/anaconda3/envs/py39/lib/python3.9/site-packages (from aiohttp->datasets[audio]) (4.0.3)\n",
      "Requirement already satisfied: certifi in /home/user1/anaconda3/envs/py39/lib/python3.9/site-packages (from httpx>=0.24.1->gradio) (2023.7.22)\n",
      "Requirement already satisfied: httpcore==1.* in /home/user1/anaconda3/envs/py39/lib/python3.9/site-packages (from httpx>=0.24.1->gradio) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/user1/anaconda3/envs/py39/lib/python3.9/site-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /home/user1/anaconda3/envs/py39/lib/python3.9/site-packages (from importlib-resources<7.0,>=1.3->gradio) (3.11.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /home/user1/anaconda3/envs/py39/lib/python3.9/site-packages (from markdown>=2.6.8->tensorboard) (6.0.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/user1/anaconda3/envs/py39/lib/python3.9/site-packages (from matplotlib~=3.0->gradio) (1.1.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/user1/anaconda3/envs/py39/lib/python3.9/site-packages (from matplotlib~=3.0->gradio) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/user1/anaconda3/envs/py39/lib/python3.9/site-packages (from matplotlib~=3.0->gradio) (4.43.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/user1/anaconda3/envs/py39/lib/python3.9/site-packages (from matplotlib~=3.0->gradio) (1.4.5)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/user1/anaconda3/envs/py39/lib/python3.9/site-packages (from matplotlib~=3.0->gradio) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/user1/anaconda3/envs/py39/lib/python3.9/site-packages (from matplotlib~=3.0->gradio) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/user1/anaconda3/envs/py39/lib/python3.9/site-packages (from pandas->datasets[audio]) (2023.3.post1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /home/user1/anaconda3/envs/py39/lib/python3.9/site-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in /home/user1/anaconda3/envs/py39/lib/python3.9/site-packages (from pydantic>=2.0->gradio) (2.20.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/user1/anaconda3/envs/py39/lib/python3.9/site-packages (from requests>=2.32.2->datasets[audio]) (2.0.4)\n",
      "Requirement already satisfied: cffi>=1.0 in /home/user1/anaconda3/envs/py39/lib/python3.9/site-packages (from soundfile>=0.12.1->datasets[audio]) (1.15.1)\n",
      "Requirement already satisfied: sympy in /home/user1/anaconda3/envs/py39/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (1.12)\n",
      "Requirement already satisfied: networkx in /home/user1/anaconda3/envs/py39/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (3.2.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/user1/anaconda3/envs/py39/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/user1/anaconda3/envs/py39/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/user1/anaconda3/envs/py39/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/user1/anaconda3/envs/py39/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/user1/anaconda3/envs/py39/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/user1/anaconda3/envs/py39/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/user1/anaconda3/envs/py39/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/user1/anaconda3/envs/py39/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/user1/anaconda3/envs/py39/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /home/user1/anaconda3/envs/py39/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (2.18.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/user1/anaconda3/envs/py39/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: triton==2.1.0 in /home/user1/anaconda3/envs/py39/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (2.1.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/user1/anaconda3/envs/py39/lib/python3.9/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.3.52)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /home/user1/anaconda3/envs/py39/lib/python3.9/site-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /home/user1/anaconda3/envs/py39/lib/python3.9/site-packages (from typer<1.0,>=0.12->gradio) (13.6.0)\n",
      "Requirement already satisfied: starlette<0.38.0,>=0.37.2 in /home/user1/anaconda3/envs/py39/lib/python3.9/site-packages (from fastapi->gradio) (0.37.2)\n",
      "Requirement already satisfied: audioread>=2.1.9 in /home/user1/anaconda3/envs/py39/lib/python3.9/site-packages (from librosa->datasets[audio]) (3.0.1)\n",
      "Requirement already satisfied: scipy>=1.2.0 in /home/user1/anaconda3/envs/py39/lib/python3.9/site-packages (from librosa->datasets[audio]) (1.11.3)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in /home/user1/anaconda3/envs/py39/lib/python3.9/site-packages (from librosa->datasets[audio]) (1.5.1)\n",
      "Requirement already satisfied: joblib>=0.14 in /home/user1/anaconda3/envs/py39/lib/python3.9/site-packages (from librosa->datasets[audio]) (1.4.2)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /home/user1/anaconda3/envs/py39/lib/python3.9/site-packages (from librosa->datasets[audio]) (4.4.2)\n",
      "Requirement already satisfied: numba>=0.51.0 in /home/user1/anaconda3/envs/py39/lib/python3.9/site-packages (from librosa->datasets[audio]) (0.60.0)\n",
      "Requirement already satisfied: pooch>=1.1 in /home/user1/anaconda3/envs/py39/lib/python3.9/site-packages (from librosa->datasets[audio]) (1.8.2)\n",
      "Requirement already satisfied: soxr>=0.3.2 in /home/user1/anaconda3/envs/py39/lib/python3.9/site-packages (from librosa->datasets[audio]) (0.4.0)\n",
      "Requirement already satisfied: lazy-loader>=0.1 in /home/user1/anaconda3/envs/py39/lib/python3.9/site-packages (from librosa->datasets[audio]) (0.4)\n",
      "Requirement already satisfied: msgpack>=1.0 in /home/user1/anaconda3/envs/py39/lib/python3.9/site-packages (from librosa->datasets[audio]) (1.0.8)\n",
      "Requirement already satisfied: pycparser in /home/user1/anaconda3/envs/py39/lib/python3.9/site-packages (from cffi>=1.0->soundfile>=0.12.1->datasets[audio]) (2.21)\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /home/user1/anaconda3/envs/py39/lib/python3.9/site-packages (from numba>=0.51.0->librosa->datasets[audio]) (0.43.0)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in /home/user1/anaconda3/envs/py39/lib/python3.9/site-packages (from pooch>=1.1->librosa->datasets[audio]) (3.10.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/user1/anaconda3/envs/py39/lib/python3.9/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/user1/anaconda3/envs/py39/lib/python3.9/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.15.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/user1/anaconda3/envs/py39/lib/python3.9/site-packages (from scikit-learn>=0.20.0->librosa->datasets[audio]) (3.5.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mpmath>=0.19 in /home/user1/anaconda3/envs/py39/lib/python3.9/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\r\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/user1/anaconda3/envs/py39/lib/python3.9/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install --upgrade datasets[audio] transformers accelerate evaluate jiwer tensorboard gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98077e1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Aug  6 11:41:58 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 545.23.08              Driver Version: 545.23.08    CUDA Version: 12.3     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA TITAN RTX               Off | 00000000:0A:00.0 Off |                  N/A |\n",
      "| 23%   46C    P0              37W / 280W |      0MiB / 24576MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "gpu_info = !nvidia-smi\n",
    "gpu_info = '\\n'.join(gpu_info)\n",
    "if gpu_info.find('failed') >= 0:\n",
    "  print('Not connected to a GPU')\n",
    "else:\n",
    "  print(gpu_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bfda9938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface_hub in /home/user1/anaconda3/envs/py39/lib/python3.9/site-packages (0.24.5)\n",
      "Requirement already satisfied: filelock in /home/user1/anaconda3/envs/py39/lib/python3.9/site-packages (from huggingface_hub) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/user1/anaconda3/envs/py39/lib/python3.9/site-packages (from huggingface_hub) (2023.10.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/user1/anaconda3/envs/py39/lib/python3.9/site-packages (from huggingface_hub) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/user1/anaconda3/envs/py39/lib/python3.9/site-packages (from huggingface_hub) (6.0.1)\n",
      "Requirement already satisfied: requests in /home/user1/anaconda3/envs/py39/lib/python3.9/site-packages (from huggingface_hub) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/user1/anaconda3/envs/py39/lib/python3.9/site-packages (from huggingface_hub) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/user1/anaconda3/envs/py39/lib/python3.9/site-packages (from huggingface_hub) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/user1/anaconda3/envs/py39/lib/python3.9/site-packages (from requests->huggingface_hub) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/user1/anaconda3/envs/py39/lib/python3.9/site-packages (from requests->huggingface_hub) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/user1/anaconda3/envs/py39/lib/python3.9/site-packages (from requests->huggingface_hub) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/user1/anaconda3/envs/py39/lib/python3.9/site-packages (from requests->huggingface_hub) (2023.7.22)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b4095913ea24332b4b46b7988b3d6a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "!pip install huggingface_hub\n",
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95eedaed",
   "metadata": {},
   "source": [
    "# Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8001d6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user1/anaconda3/envs/py39/lib/python3.9/site-packages/datasets/load.py:2554: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.\n",
      "You can remove this warning by passing 'token=<use_auth_token>' instead.\n",
      "  warnings.warn(\n",
      "/home/user1/anaconda3/envs/py39/lib/python3.9/site-packages/datasets/load.py:2554: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.\n",
      "You can remove this warning by passing 'token=<use_auth_token>' instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['client_id', 'path', 'audio', 'sentence', 'up_votes', 'down_votes', 'age', 'gender', 'accent', 'locale', 'segment', 'variant'],\n",
      "        num_rows: 7117\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['client_id', 'path', 'audio', 'sentence', 'up_votes', 'down_votes', 'age', 'gender', 'accent', 'locale', 'segment', 'variant'],\n",
      "        num_rows: 3154\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "common_voice = DatasetDict()\n",
    "\n",
    "common_voice[\"train\"] = load_dataset(\"mozilla-foundation/common_voice_17_0\", \"hi\", split=\"train+validation\", use_auth_token=True)\n",
    "common_voice[\"test\"] = load_dataset(\"mozilla-foundation/common_voice_17_0\", \"hi\", split=\"test\", use_auth_token=True)\n",
    "\n",
    "print(common_voice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70d51ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_voice = common_voice.remove_columns([\"accent\", \"age\", \"client_id\", \"down_votes\", \"gender\", \"locale\", \"path\", \"segment\", \"up_votes\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d9e974a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperFeatureExtractor\n",
    "\n",
    "feature_extractor = WhisperFeatureExtractor.from_pretrained(\"openai/whisper-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b71293e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperTokenizer\n",
    "\n",
    "tokenizer = WhisperTokenizer.from_pretrained(\"openai/whisper-small\", language=\"Hindi\", task=\"transcribe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "61e0167f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:                 हमने उसका जन्मदिन मनाया।\n",
      "Decoded w/ special:    <|startoftranscript|><|hi|><|transcribe|><|notimestamps|>हमने उसका जन्मदिन मनाया।<|endoftext|>\n",
      "Decoded w/out special: हमने उसका जन्मदिन मनाया।\n",
      "Are equal:             True\n"
     ]
    }
   ],
   "source": [
    "input_str = common_voice[\"train\"][0][\"sentence\"]\n",
    "labels = tokenizer(input_str).input_ids\n",
    "decoded_with_special = tokenizer.decode(labels, skip_special_tokens=False)\n",
    "decoded_str = tokenizer.decode(labels, skip_special_tokens=True)\n",
    "\n",
    "print(f\"Input:                 {input_str}\")\n",
    "print(f\"Decoded w/ special:    {decoded_with_special}\")\n",
    "print(f\"Decoded w/out special: {decoded_str}\")\n",
    "print(f\"Are equal:             {input_str == decoded_str}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "61d5f61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperProcessor\n",
    "\n",
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-small\", language=\"Hindi\", task=\"transcribe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b6909c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'audio': {'path': '/home/user1/.cache/huggingface/datasets/downloads/extracted/3c68a22dcb7bd2b170750da824b605d17ad6a53eaba4fa853c22bc197bd89de9/hi_train_0/common_voice_hi_26008353.mp3', 'array': array([ 6.46234854e-26, -1.35709319e-25, -8.07793567e-26, ...,\n",
      "        1.06425944e-07,  4.46417090e-08,  2.61451660e-09]), 'sampling_rate': 48000}, 'sentence': 'हमने उसका जन्मदिन मनाया।', 'variant': ''}\n"
     ]
    }
   ],
   "source": [
    "print(common_voice[\"train\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "83eb96c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Audio\n",
    "\n",
    "common_voice = common_voice.cast_column(\"audio\", Audio(sampling_rate=16000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5dab3d61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'audio': {'path': '/home/user1/.cache/huggingface/datasets/downloads/extracted/3c68a22dcb7bd2b170750da824b605d17ad6a53eaba4fa853c22bc197bd89de9/hi_train_0/common_voice_hi_26008353.mp3', 'array': array([ 5.98479599e-17,  3.12250226e-17, -1.04083409e-17, ...,\n",
      "       -1.31181878e-07,  2.62807589e-07,  4.75192792e-08]), 'sampling_rate': 16000}, 'sentence': 'हमने उसका जन्मदिन मनाया।', 'variant': ''}\n"
     ]
    }
   ],
   "source": [
    "print(common_voice[\"train\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c6d74218",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(batch):\n",
    "    # load and resample audio data from 48 to 16kHz\n",
    "    audio = batch[\"audio\"]\n",
    "\n",
    "    # compute log-Mel input features from input audio array\n",
    "    batch[\"input_features\"] = feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_features[0]\n",
    "\n",
    "    # encode target text to label ids\n",
    "    batch[\"labels\"] = tokenizer(batch[\"sentence\"]).input_ids\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "38b736c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50f1865d5ed04bb4a5f518d50aa3bf78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7117 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58a296cf815442d2af0935345949372c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3154 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "common_voice = common_voice.map(prepare_dataset, remove_columns=common_voice.column_names[\"train\"], num_proc=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6227b569",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d5f02616c6948a6aa2f4624f8b35002",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.97k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb46125fae23487aa429afd4d8872c07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/967M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "326fe76ec424423cadb59a3d2d5218c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/3.87k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import WhisperForConditionalGeneration\n",
    "\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c65143c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.generation_config.language = \"hindi\"\n",
    "model.generation_config.task = \"transcribe\"\n",
    "\n",
    "model.generation_config.forced_decoder_ids = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8871c541",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: Any\n",
    "    decoder_start_token_id: int\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lengths and need different padding methods\n",
    "        # first treat the audio inputs by simply returning torch tensors\n",
    "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "\n",
    "        # get the tokenized label sequences\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "        # pad the labels to max length\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        # if bos token is appended in previous tokenization step,\n",
    "        # cut bos token here as it's append later anyways\n",
    "        if (labels[:, 0] == self.decoder_start_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0c692c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(\n",
    "    processor=processor,\n",
    "    decoder_start_token_id=model.config.decoder_start_token_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a104d5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"wer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1fb80ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    pred_ids = pred.predictions\n",
    "    label_ids = pred.label_ids\n",
    "\n",
    "    # replace -100 with the pad_token_id\n",
    "    label_ids[label_ids == -100] = tokenizer.pad_token_id\n",
    "\n",
    "    # we do not want to group tokens when computing the metrics\n",
    "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
    "\n",
    "    wer = 100 * metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return {\"wer\": wer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "85d1ba4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments\n",
    "import torch\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./whisper-small-hi\",\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    gradient_accumulation_steps=16,\n",
    "    learning_rate=1e-5,\n",
    "    warmup_steps=100,\n",
    "    max_steps=390,\n",
    "    gradient_checkpointing=True,\n",
    "    fp16=True,\n",
    "    save_strategy=\"epoch\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=225,\n",
    "    save_steps=78,\n",
    "    eval_steps=1,\n",
    "    logging_steps=20,\n",
    "    report_to=[\"tensorboard\"],\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"wer\",\n",
    "    greater_is_better=False,\n",
    "    push_to_hub=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bd1d8be1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "from transformers import Seq2SeqTrainer\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    args=training_args,\n",
    "    model=model,\n",
    "    train_dataset=common_voice[\"train\"],\n",
    "    eval_dataset=common_voice[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=processor.feature_extractor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "783f939c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user1/anaconda3/envs/py39/lib/python3.9/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='390' max='390' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [390/390 8:11:13, Epoch 14/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Wer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.773100</td>\n",
       "      <td>0.589453</td>\n",
       "      <td>60.338633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.496400</td>\n",
       "      <td>0.410128</td>\n",
       "      <td>45.715487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.261300</td>\n",
       "      <td>0.341060</td>\n",
       "      <td>40.635988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.203200</td>\n",
       "      <td>0.315535</td>\n",
       "      <td>37.394894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.162200</td>\n",
       "      <td>0.308061</td>\n",
       "      <td>36.064822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.100100</td>\n",
       "      <td>0.312635</td>\n",
       "      <td>35.441828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.082600</td>\n",
       "      <td>0.326465</td>\n",
       "      <td>35.476227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.054100</td>\n",
       "      <td>0.340084</td>\n",
       "      <td>35.334811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.041800</td>\n",
       "      <td>0.352788</td>\n",
       "      <td>35.392142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.035000</td>\n",
       "      <td>0.366848</td>\n",
       "      <td>35.438006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.024500</td>\n",
       "      <td>0.378278</td>\n",
       "      <td>35.629109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.021200</td>\n",
       "      <td>0.387985</td>\n",
       "      <td>36.030423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.017200</td>\n",
       "      <td>0.394152</td>\n",
       "      <td>35.824033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.015900</td>\n",
       "      <td>0.395579</td>\n",
       "      <td>35.915762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.015900</td>\n",
       "      <td>0.395613</td>\n",
       "      <td>35.908118</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You have passed task=transcribe, but also have set `forced_decoder_ids` to [[1, 50259], [2, 50359], [3, 50363]] which creates a conflict. `forced_decoder_ids` will be ignored in favor of task=transcribe.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token.As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}\n",
      "/home/user1/anaconda3/envs/py39/lib/python3.9/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}\n",
      "/home/user1/anaconda3/envs/py39/lib/python3.9/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}\n",
      "/home/user1/anaconda3/envs/py39/lib/python3.9/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}\n",
      "/home/user1/anaconda3/envs/py39/lib/python3.9/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}\n",
      "/home/user1/anaconda3/envs/py39/lib/python3.9/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}\n",
      "/home/user1/anaconda3/envs/py39/lib/python3.9/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}\n",
      "/home/user1/anaconda3/envs/py39/lib/python3.9/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}\n",
      "/home/user1/anaconda3/envs/py39/lib/python3.9/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}\n",
      "/home/user1/anaconda3/envs/py39/lib/python3.9/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}\n",
      "/home/user1/anaconda3/envs/py39/lib/python3.9/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}\n",
      "/home/user1/anaconda3/envs/py39/lib/python3.9/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}\n",
      "/home/user1/anaconda3/envs/py39/lib/python3.9/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}\n",
      "/home/user1/anaconda3/envs/py39/lib/python3.9/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}\n",
      "There were missing keys in the checkpoint model loaded: ['proj_out.weight'].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=390, training_loss=0.1487852399929976, metrics={'train_runtime': 29515.3732, 'train_samples_per_second': 3.383, 'train_steps_per_second': 0.013, 'total_flos': 2.880024578113536e+19, 'train_loss': 0.1487852399929976, 'epoch': 14.02247191011236})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "aa40d308",
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {\n",
    "    \"dataset_tags\": \"mozilla-foundation/common_voice_17_0\",\n",
    "    \"dataset\": \"Common Voice 17.0\",\n",
    "    \"dataset_args\": \"config: hi, split: test\",\n",
    "    \"language\": \"hi\",\n",
    "    \"model_name\": \"Whisper Small Hi - Rishabh Mathur\",  # a 'pretty' name for your model\n",
    "    \"finetuned_from\": \"openai/whisper-small\",\n",
    "    \"tasks\": \"automatic-speech-recognition\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "695a23df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b317cb928cd4819ad666ab6512dd7da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training_args.bin:   0%|          | 0.00/5.37k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "174c964a934a44fd8af3bb2a9936aa06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 5 LFS files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db25dc7d40c545ce8f3ab6309fca12d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "events.out.tfevents.1722925315.node1.14804.0:   0%|          | 0.00/6.67k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6615173679cd42a8b5b9595c5c5e0612",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/967M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4812c65efa3430890632e3c7c8b22d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "events.out.tfevents.1722925917.node1.14804.1:   0%|          | 0.00/6.67k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "778ecb26f24742c0baacd76b774915e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "events.out.tfevents.1722926839.node1.14804.2:   0%|          | 0.00/15.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/Rishabh06/whisper-small-hi/commit/d06a97fb7b70df9fde815f656388c2da4a897ec8', commit_message='End of training', commit_description='', oid='d06a97fb7b70df9fde815f656388c2da4a897ec8', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.push_to_hub(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f59fb514",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import WhisperTokenizer, WhisperFeatureExtractor, WhisperProcessor\n",
    "\n",
    "# Load the pretrained tokenizer and feature extractor\n",
    "tokenizer = WhisperTokenizer.from_pretrained(\"openai/whisper-small\")\n",
    "feature_extractor = WhisperFeatureExtractor.from_pretrained(\"openai/whisper-small\")\n",
    "\n",
    "# Combine them into a processor with the correct order\n",
    "processor = WhisperProcessor(feature_extractor, tokenizer)\n",
    "\n",
    "# Save the processor locally\n",
    "processor.save_pretrained(\"./whisper-small-hi-processor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5e8a59a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/Rishabh06/whisper-small-hi/commit/6fb15e77b7dbb84f43b43ed97f4aed43db046c83', commit_message='Upload folder using huggingface_hub', commit_description='', oid='6fb15e77b7dbb84f43b43ed97f4aed43db046c83', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import HfApi\n",
    "\n",
    "# Initialize the Hugging Face API\n",
    "api = HfApi()\n",
    "\n",
    "# Upload the processor folder to your model repository\n",
    "api.upload_folder(\n",
    "    folder_path=\"./whisper-small-hi-processor\",\n",
    "    path_in_repo=\"\",\n",
    "    repo_id=\"Rishabh06/whisper-small-hi\",\n",
    "    repo_type=\"model\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8b270aa4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d7b0c7d2c73415aa2492ffe95ebb219",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/283k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f6a260bb428412d8730b1bdf561e5e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af8a1f7268b44dca8e6cb9ac1df6cf9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/494k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6b80823049c4ea493ecaa3ed4a0cad0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "normalizer.json:   0%|          | 0.00/52.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "501dbac633724fbfa49fe24ba9439b88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/34.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cf23ba3826a4fd89b7ec40393955ad7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/2.19k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import WhisperForConditionalGeneration, WhisperProcessor\n",
    "\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"Rishabh06/whisper-small-hi\")\n",
    "processor = WhisperProcessor.from_pretrained(\"Rishabh06/whisper-small-hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc624c2a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
